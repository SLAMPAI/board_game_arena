{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Game Reasoning Arena - Simple Tic-Tac-Toe Demo\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/lcipolina/open_spiel_arena/blob/main/colabs/LLM_OpenSpiel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook demonstrates how to use the **Game Reasoning Arena** framework to run a simple tic-tac-toe game between a lightweight Large Language Model (LLM) and a random bot.\n",
        "\n",
        "## Features\n",
        "- Uses the Game Reasoning Arena framework\n",
        "- Lightweight LLM vs Random bot gameplay\n",
        "- Clean and simple demonstration\n",
        "- Real game logging and analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXOavIGokPyi"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install openspiel litellm pyyaml\n",
        "\n",
        "# Clone the Game Reasoning Arena repository\n",
        "!git clone https://github.com/SLAMPAI/game_reasoning_arena.git\n",
        "%cd game_reasoning_arena\n",
        "\n",
        "# Install the package\n",
        "!pip install .\n",
        "\n",
        "# Set up environment\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/game_reasoning_arena/src')\n",
        "\n",
        "print(\"Game Reasoning Arena setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i10d2cM-mbsI"
      },
      "outputs": [],
      "source": [
        "# Import Game Reasoning Arena components\n",
        "from game_reasoning_arena.arena.games.registry import registry\n",
        "from game_reasoning_arena.arena.agents.policy_manager import initialize_policies\n",
        "from game_reasoning_arena.backends import initialize_llm_registry\n",
        "from game_reasoning_arena.arena.utils.seeding import set_seed\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"Imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HmsUys-2OoL",
        "outputId": "e0d2823d-fd3f-4b6c-b864-39ab50b53ea2"
      },
      "outputs": [],
      "source": [
        "# Configuration for a simple tic-tac-toe game\n",
        "config = {\n",
        "    \"env_configs\": [\n",
        "        {\n",
        "            \"game_name\": \"tic_tac_toe\",\n",
        "            \"max_game_rounds\": None\n",
        "        }\n",
        "    ],\n",
        "    \"num_episodes\": 3,  # Run 3 games\n",
        "    \"seed\": 42,\n",
        "    \"use_ray\": False,\n",
        "    \"mode\": \"llm_vs_random\",\n",
        "    \"agents\": {\n",
        "        \"player_0\": {\n",
        "            \"type\": \"llm\",\n",
        "            \"model\": \"litellm_groq/gemma-7b-it\"  # Lightweight model\n",
        "        },\n",
        "        \"player_1\": {\n",
        "            \"type\": \"random\"\n",
        "        }\n",
        "    },\n",
        "    \"llm_backend\": {\n",
        "        \"max_tokens\": 50,\n",
        "        \"temperature\": 0.1,\n",
        "        \"default_model\": \"litellm_groq/gemma-7b-it\"\n",
        "    },\n",
        "    \"log_level\": \"INFO\"\n",
        "}\n",
        "\n",
        "def run_tic_tac_toe_demo(config):\n",
        "    \"\"\"Run a simple tic-tac-toe demonstration.\"\"\"\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    set_seed(config[\"seed\"])\n",
        "\n",
        "    # Initialize LLM backends\n",
        "    initialize_llm_registry(config)\n",
        "\n",
        "    # Get the game configuration\n",
        "    env_config = config[\"env_configs\"][0]\n",
        "    game_name = env_config[\"game_name\"]\n",
        "\n",
        "    print(f\" Starting {game_name.replace('_', ' ').title()} Demo\")\n",
        "    print(f\" LLM Model: {config['agents']['player_0']['model']}\")\n",
        "    print(f\" Random Bot vs LLM\")\n",
        "    print(f\" Episodes: {config['num_episodes']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Create environment\n",
        "    env = registry.make_env(game_name, config)\n",
        "\n",
        "    # Initialize agent policies\n",
        "    policies_dict = initialize_policies(config, game_name, config[\"seed\"])\n",
        "\n",
        "    # Mapping from player IDs to agents\n",
        "    player_to_agent = {0: policies_dict[\"policy_0\"], 1: policies_dict[\"policy_1\"]}\n",
        "\n",
        "    episode_results = []\n",
        "\n",
        "    for episode in range(config[\"num_episodes\"]):\n",
        "        print(f\"\\nðŸŽ¯ Episode {episode + 1}\")\n",
        "        print(\"=\" * 30)\n",
        "\n",
        "        # Reset environment\n",
        "        observation_dict, _ = env.reset(seed=config[\"seed\"] + episode)\n",
        "\n",
        "        # Game variables\n",
        "        episode_rewards = {0: 0, 1: 0}\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        step_count = 0\n",
        "\n",
        "        while not (terminated or truncated):\n",
        "            step_count += 1\n",
        "            print(f\"\\n Step {step_count}\")\n",
        "\n",
        "            # Show current board state\n",
        "            print(\"Current board:\")\n",
        "            print(env.render_board(0))\n",
        "\n",
        "            # Determine which player(s) should act\n",
        "            if env.state.is_simultaneous_node():\n",
        "                # All players act simultaneously (not typical for tic-tac-toe)\n",
        "                active_players = list(player_to_agent.keys())\n",
        "            else:\n",
        "                # Turn-based: only current player acts\n",
        "                current_player = env.state.current_player()\n",
        "                active_players = [current_player]\n",
        "                print(f\"Player {current_player}'s turn ({'LLM' if current_player == 0 else 'Random Bot'})\")\n",
        "\n",
        "            # Compute actions for active players\n",
        "            action_dict = {}\n",
        "            for player_id in active_players:\n",
        "                agent = player_to_agent[player_id]\n",
        "                observation = observation_dict[player_id]\n",
        "\n",
        "                # Get action from agent\n",
        "                action_result = agent.compute_action(observation)\n",
        "\n",
        "                if isinstance(action_result, dict):\n",
        "                    action = action_result.get(\"action\")\n",
        "                    reasoning = action_result.get(\"reasoning\", \"No reasoning provided\")\n",
        "                else:\n",
        "                    action = action_result\n",
        "                    reasoning = \"Random choice\"\n",
        "\n",
        "                action_dict[player_id] = action\n",
        "\n",
        "                # Log the action\n",
        "                agent_type = \"LLM\" if player_id == 0 else \"Random Bot\"\n",
        "                print(f\"  {agent_type} chooses action {action}\")\n",
        "                if player_id == 0 and reasoning:\n",
        "                    print(f\"  Reasoning: {reasoning[:100]}...\")\n",
        "\n",
        "            # Take environment step\n",
        "            observation_dict, rewards, terminated, truncated, info = env.step(action_dict)\n",
        "\n",
        "            # Update episode rewards\n",
        "            for player_id, reward in rewards.items():\n",
        "                episode_rewards[player_id] += reward\n",
        "\n",
        "        # Episode finished\n",
        "        print(f\"\\n Episode {episode + 1} Complete!\")\n",
        "        print(\"Final board:\")\n",
        "        print(env.render_board(0))\n",
        "\n",
        "        # Determine winner\n",
        "        if episode_rewards[0] > episode_rewards[1]:\n",
        "            winner = \"LLM (Player 0)\"\n",
        "        elif episode_rewards[1] > episode_rewards[0]:\n",
        "            winner = \"Random Bot (Player 1)\"\n",
        "        else:\n",
        "            winner = \"Draw\"\n",
        "\n",
        "        print(f\" Winner: {winner}\")\n",
        "        print(f\" Scores: LLM={episode_rewards[0]}, Random={episode_rewards[1]}\")\n",
        "\n",
        "        episode_results.append({\n",
        "            \"episode\": episode + 1,\n",
        "            \"winner\": winner,\n",
        "            \"llm_score\": episode_rewards[0],\n",
        "            \"random_score\": episode_rewards[1],\n",
        "            \"steps\": step_count\n",
        "        })\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" TOURNAMENT SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    llm_wins = sum(1 for r in episode_results if r[\"llm_score\"] > r[\"random_score\"])\n",
        "    random_wins = sum(1 for r in episode_results if r[\"random_score\"] > r[\"llm_score\"])\n",
        "    draws = sum(1 for r in episode_results if r[\"llm_score\"] == r[\"random_score\"])\n",
        "\n",
        "    print(f\"LLM Wins: {llm_wins}/{config['num_episodes']} ({llm_wins/config['num_episodes']*100:.1f}%)\")\n",
        "    print(f\"Random Wins: {random_wins}/{config['num_episodes']} ({random_wins/config['num_episodes']*100:.1f}%)\")\n",
        "    print(f\"Draws: {draws}/{config['num_episodes']} ({draws/config['num_episodes']*100:.1f}%)\")\n",
        "\n",
        "    return episode_results\n",
        "\n",
        "# Run the demo\n",
        "print(\" Starting Game Reasoning Arena Tic-Tac-Toe Demo\")\n",
        "print(\"This demonstrates how to use the framework for simple LLM vs Random gameplay\")\n",
        "\n",
        "# Note: You'll need to set up API keys for the LLM to work\n",
        "# For Groq: set GROQ_API_KEY environment variable\n",
        "print(\"\\n  Note: Make sure to set up your API keys for the LLM to work!\")\n",
        "print(\"For Groq models, set: GROQ_API_KEY environment variable\")\n",
        "\n",
        "try:\n",
        "    results = run_tic_tac_toe_demo(config)\n",
        "    print(\"\\n Demo completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n Demo failed: {e}\")\n",
        "    print(\"This might be due to missing API keys or network issues.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ”‘ API Key Setup - REQUIRED for LLM vs Random gameplay\n",
        "import os\n",
        "\n",
        "print(\" API Key Setup Instructions:\")\n",
        "print(\"1. Get a free Groq API key from: https://console.groq.com/\")\n",
        "print(\"2. Uncomment and set your API key below:\")\n",
        "print(\"3. Then run the LLM demo in the previous cell\")\n",
        "print()\n",
        "\n",
        "# UNCOMMENT THE LINE BELOW AND ADD YOUR API KEY:\n",
        "# os.environ[\"GROQ_API_KEY\"] = \"gsk_your_actual_groq_api_key_here\"\n",
        "\n",
        "# You can also set other provider keys if needed:\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your_openai_key_here\"\n",
        "# os.environ[\"TOGETHER_API_KEY\"] = \"your_together_ai_key_here\"\n",
        "\n",
        "print(\" Tip: Keep your API keys secure and never commit them to public repositories!\")\n",
        "print()\n",
        "\n",
        "# Fallback demo without API keys - uses random vs random\n",
        "print(\" Running fallback demo (Random vs Random) to test framework:\")\n",
        "\n",
        "fallback_config = {\n",
        "    \"env_configs\": [{\"game_name\": \"tic_tac_toe\", \"max_game_rounds\": None}],\n",
        "    \"num_episodes\": 2,\n",
        "    \"seed\": 42,\n",
        "    \"use_ray\": False,\n",
        "    \"mode\": \"random_vs_random\",\n",
        "    \"agents\": {\n",
        "        \"player_0\": {\"type\": \"random\"},\n",
        "        \"player_1\": {\"type\": \"random\"}\n",
        "    },\n",
        "    \"log_level\": \"INFO\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    fallback_results = run_tic_tac_toe_demo(fallback_config)\n",
        "    print(\"\\n Framework test completed - everything is working!\")\n",
        "    print(\" Now set your API key above to try LLM vs Random!\")\n",
        "except Exception as e:\n",
        "    print(f\" Framework setup issue: {e}\")\n",
        "    print(\"Please check the installation in the previous cells.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOW44zwOY5XCupCGP8UiQ+O",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "game_reasoning_arena",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
