# Hybrid configuration - LiteLLM vs vLLM
# Demonstrates mixed backends: API-based vs local GPU inference

# Use env_config as a single dict for single games
env_config:
  game_name: tic_tac_toe
  max_game_rounds: null

num_episodes: 1
seed: 42
use_ray: false
mode: llm_vs_llm

agents:
  player_0:
    type: llm
    model: litellm_groq/llama3-8b-8192  # API-based model
  player_1:
    type: llm
    model: vllm_Qwen2-7B-Instruct      # Local GPU model

llm_backend:
  max_tokens: 250
  temperature: 0.1
  default_model: litellm_groq/llama3-8b-8192

log_level: INFO
